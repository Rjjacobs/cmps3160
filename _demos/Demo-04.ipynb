{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 04 - Pandas, Data Tables, and Tidy Data\n",
    "\n",
    "In this demo we will go over some of the basics of Pandas, so that you have examples of the basic functionality for reference. We then move into working with those data tables, a deep dive on the GroupBy command and how it works, and finish with another deep dive on Melting data and the concepts of Tidy Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, mount your google drive, change to the course folder, pull latest changes, and change to the lab folder.\n",
    "# Startup Magic to: (1) Mount Google Drive\n",
    "# (2) Change to Course Folder\n",
    "# (3) Pull latest Changes\n",
    "# (4) Move to the Demo Directory so that the data files are available\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/My Drive/cmps3160\n",
    "!git pull\n",
    "%cd _demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes and Standard Magic...\n",
    "### Standard Magic and startup initializers.\n",
    "\n",
    "# Load Numpy\n",
    "import numpy as np\n",
    "# Load MatPlotLib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Load Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# This lets us show plots inline and also save PDF plots if we want them\n",
    "%matplotlib inline\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "matplotlib.style.use('fivethirtyeight')\n",
    "\n",
    "# These two things are for Pandas, it widens the notebook and lets us display data easily.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Show a ludicrus number of rows and columns\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pandas Functionality\n",
    "\n",
    "### Creating Tables\n",
    "\n",
    "First, let's take a look at some basic Pandas functionality. These are small examples to show the ideas of creating, selecting from, and working with data tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.DataFrame([0.3, 0.8, 0.1, 4.0, 1.2], \n",
    "                  index = ['a', 'c', 'd', 'e', 'f'], \n",
    "                  columns=['data'])\n",
    "\n",
    "display(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = pd.DataFrame([0.9, 0.1, 4.8, 0.3], \n",
    "                  index = ['b', 'c', 'd', 'g'], \n",
    "                  columns=['data'])\n",
    "display(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-index can get used to do some funky things or make bigger frames.. this is not common usage but you use reindex to make new entries in an already existing table.  [Doc Page](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reindex.html)\n",
    "\n",
    "```Places NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use re-index to fill in some gaps if we want..\n",
    "import string\n",
    "\n",
    "display(s1.reindex(list(string.ascii_lowercase)[:10]))\n",
    "\n",
    "# We can also use the 'ffill' method to fill the missing data with the closest data.\n",
    "display(s1.reindex(list(string.ascii_lowercase)[:10], method='ffill'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above operation did not happen in place! so that our s1 table is preserved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering, Selecting, and Setting Data.\n",
    "\n",
    "The first question we might have is how do we get at data in our dataframe? The easiest way is to get a location by it's `index` or `label`. To do this we use the `.loc` command. We can use this command to slice by the index as well!\n",
    "\n",
    "For more on the difference between loc and iloc see the [10 Mins to Pandas](https://pandas.pydata.org/docs/user_guide/10min.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection by location or loc\n",
    "\n",
    "s1.loc['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.loc['a':'d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.loc['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `iloc` command to get a row by it's **interger position**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection by position, iloc\n",
    "s2.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(s1.iloc[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gotcha Warning!** Note that loc is inclusive while iloc is not inclusive at the end of the range -- just like indexing into an array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(s1)\n",
    "display(s1.iloc[1:4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use set data in a data frame by using the `at` and `iat` commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(s1)\n",
    "s1.at['a','data'] = 0\n",
    "display(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(s1)\n",
    "s1.iat[2,0] = 0\n",
    "display(s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both of the above operations happened inplace!! **Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reset our data frames since we messed them up!\n",
    "\n",
    "s1 = pd.DataFrame([0.3, 0.8, 0.1, 4.0, 1.2], \n",
    "                  index = ['a', 'c', 'd', 'e', 'f'], \n",
    "                  columns=['data'])\n",
    "\n",
    "s2 = pd.DataFrame([0.9, 0.1, 4.8, 0.3], \n",
    "                  index = ['b', 'c', 'd', 'g'], \n",
    "                  columns=['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall we can also use aggregation operators over the columns\n",
    "\n",
    "s2['data'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen we can use boolean operations to filter and select data. Recall that these operations return a **view** of the data and not a copy of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2[s2['data'] > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1[ (s1['data'] < 1.0) & (s1['data'] > 3.0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on Data Tables - Broadcasting\n",
    "\n",
    "When we add constants to a table we get something called **broadcasting** where the operation happens to every element of a table (or column!) You can read more about this essential basic functionality at the [Pandas Documentation Website](https://pandas.pydata.org/docs/user_guide/basics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above operations did not happen **inplace** so that our tables remain the same as when we declared them! When you manipulate a datatable you must always tell pandas to either save that as a new variable or table, or to do the operation in place.\n",
    "\n",
    "**Question:** What happens if we add two tables together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note again that these operations did not happen in place!\n",
    "display(s1)\n",
    "display(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 + s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 - s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that + is like what we'll learn is an inner join soon but it also adds the numbers together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also apply functions across and down and with lambdas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(np.random.randn(4,3), columns=list('abc'),\n",
    "                 index=['Utah', 'Ohio', 'Texas','Oregon'])\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = lambda x: x.max() - x.min()\n",
    "\n",
    "frame.apply(minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default is axis=0 or per column, can also do per row!\n",
    "frame.apply(minmax, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on Data Tables - Ranking\n",
    "\n",
    "Some of the [Pandas Rank](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rank.html) commands aren't totally obvious..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame({'a':[0, 1, 0, 1],\n",
    "                      'b':[4.3, 7, -3, 2],\n",
    "                   'c':[-2, 5, 8, -2.5]})\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default is average but you can do other things!\n",
    "\n",
    "display(frame.rank(ascending=False))\n",
    "\n",
    "display(frame.rank(ascending=False, method='first'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also sort along rows!\n",
    "\n",
    "frame.rank(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Hierarchical Indices\n",
    "\n",
    "To set a **hierarchical index** one can refer to the [documetnation page](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a heiararichal index -- The bad way would be to build up tuples as an index.\n",
    "\n",
    "index = [('California', 2000), ('California', 2010),\n",
    "         ('New York', 2000), ('New York', 2010),\n",
    "         ('Texas', 2000), ('Texas', 2010)]\n",
    "populations = [33871648, 37253956,\n",
    "               18976457, 19378102,\n",
    "               20851820, 25145561]\n",
    "pop = pd.Series(populations, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can still get slices etc. in a strightforward way.\n",
    "pop[('California', 2010):('Texas', 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But if we want to get all 2010 data we have to do something like...\n",
    "pop[[i for i in pop.index if i[1] == 2010]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can make the data frame as (note it is tidy!)\n",
    "df = pd.DataFrame([('California', 2000, 33871648),\n",
    "                   ('California', 2010, 37253956),\n",
    "                   ('New York', 2000, 18976457),\n",
    "                   ('New York', 2010, 19378102),\n",
    "                   ('Texas', 2000, 20851820),\n",
    "                   ('Texas', 2010, 25145561)],\n",
    "                  columns=['state', 'year', 'pop'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(['state', 'year'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we can do cool stuff slicing, but it gets compicated with tuples.\n",
    "df.loc[('California')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [df.xs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.xs.html) command can help with slicing multi-indicies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thought this can get a bit complicated... \n",
    "df.xs(2010, level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of using np.nan\n",
    "\n",
    "Note that we have to import numpy.nan but once we do we can use the groupby and other methods without having to worry about what to do with missing data.\n",
    "\n",
    "For the example below, what happens when we don't use NaNs in the data table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a data frame from a lists\n",
    "# Try replacing 'XX' with various values..\n",
    "\n",
    "df = pd.DataFrame({'age':     [12.2, 11.0, 15.6, '--'],\n",
    "                  'wgt_kg':   [42.3, 40.8, 65.3, 84.2],\n",
    "                  'hgt_cm':   [145.1, 143.8, 165.3, 185.8],\n",
    "                  'sex':      ['male', 'female', 'male', 'male'],\n",
    "                  'country': ['USA', 'AUS', 'EU', 'USA']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the type that has been imputed here...\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['age'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to make sure it's set as number!\n",
    "\n",
    "df[\"age\"] = pd.to_numeric(df[\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But first we have to make sure it's a NAN!!\n",
    "display(df[\"age\"].replace(\"--\", np.nan))\n",
    "\n",
    "df[\"age\"].replace(\"--\", np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Groupby Command\n",
    "\n",
    "Below we see what happens in Pandas when we use the groupby command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'age':     [12.2, 11.0, 15.6, 35.1],\n",
    "                  'wgt_kg':   [42.3, 40.8, 65.3, 84.2],\n",
    "                  'hgt_cm':   [145.1, 143.8, 165.3, 185.8],\n",
    "                  'sex':      ['male', 'female', 'male', 'male'],\n",
    "                  'country': ['USA', 'AUS', 'EU', 'USA']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a little weird, recall that we have lazy evaluation!\n",
    "\n",
    "df.groupby(['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we describe it then we can force python to not be lazy!\n",
    "\n",
    "df.groupby('sex').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pass operators to groupby to get better results.\n",
    "df.groupby(['sex']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also group by multiple columns\n",
    "df.groupby(['sex', 'country']).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Index!!\n",
    "# We can select based on a list of indices\n",
    "df.groupby(['sex', 'country']).mean().loc['female', 'AUS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also limit ourselves by just subselecting columns\n",
    "display(df.groupby([\"country\"]).count())\n",
    "display(df.groupby([\"country\"])[[\"sex\"]].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as...\n",
    "df['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as...\n",
    "grouped = df.groupby(['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get access to a group that we made, use the [get_group() command](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.get_group.html).\n",
    "\n",
    "We can also get access to the actual tuples and they're types that are generated by the `groupby` commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.get_group('AUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or do it fancy?\n",
    "[display(grouped.get_group(i)) for i in grouped.groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Big Deep Dive on Groupby!\n",
    "\n",
    "**Question** If we were going to implement Groupby, how would you do it?\n",
    "\n",
    "The first thing we can note is that if we iterate over the groups, it makes it clear what Pandas and Python are doing.\n",
    "\n",
    "However we don't want to iterate ourselves as this will generally be slow and error prone!\n",
    "\n",
    "**We always want to replace for loops in Pandas** since Pandas functions are highly optimized (numpy + C + vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(['country'])\n",
    "grouped.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_index, groupi in grouped:\n",
    "  print(group_index)\n",
    "  display(groupi)\n",
    "  print(groupi['age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('country')['age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split-Apply-Combine\n",
    "\n",
    "The `groupby` operation can be understood as a Split-Apply-Combine pattern:\n",
    "\n",
    "<img src=\"https://jakevdp.github.io/figures/split-apply-combine.svg\" width=600/>\n",
    "\n",
    "How could we implement this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = pd.DataFrame([\n",
    "    {'key': 'A', 'data': 1}, {'key': 'B', 'data': 2},\n",
    "    {'key': 'C', 'data': 3}, {'key': 'A', 'data': 4},\n",
    "    {'key': 'B', 'data': 5}, {'key': 'C', 'data': 6},\n",
    "  ]\n",
    ")\n",
    "dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.groupby('key').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Boolean Masks\n",
    "def group_by_masks(dfi):\n",
    "  result = []\n",
    "  for key in ['A','B', 'C']:\n",
    "    result.append({\n",
    "        'key': key,\n",
    "        'data': dfi[dfi['key']==key]['data'].sum()\n",
    "    })\n",
    "  return pd.DataFrame(result)\n",
    "\n",
    "group_by_masks(dfi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Big Oh runtime of this implementation?\n",
    "\n",
    "Can we improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Build a dict from key->sum in a single pass through data.\n",
    "from collections import defaultdict\n",
    "def group_by_dict(dfi):\n",
    "  sums = {}\n",
    "  for index, row in dfi.iterrows():\n",
    "    sums[row['key']] = sums.get(row['key'], 0) + row['data']\n",
    "  # display(sums)\n",
    "  return pd.DataFrame.from_dict(sums, orient='index', columns=['data'])\n",
    "  return sums\n",
    "group_by_dict(dfi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy evaluation\n",
    "\n",
    "- `groupby` doesn't actually do much until you do something with the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a big DataFrame.\n",
    "bigdfi = dfi.sample(100000, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call groupby, but don't do anything with it.\n",
    "%timeit bigdfi.groupby('key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once you use the groups, it takes a lot more time.\n",
    "%timeit bigdfi.groupby('key').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison, this is the time to just sum the data column.\n",
    "%timeit bigdfi.data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas uses Cython internally, which implements core python functions \n",
    "# # in highly optimized C code.\n",
    "# This is one reason why our implementation is horribly slow.\n",
    "%timeit group_by_dict(bigdfi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our mask implementation relies on pandas functions, so it is fast,\n",
    "# even though the asymptotic runtime should be larger than group_by_dict!\n",
    "%timeit group_by_masks(bigdfi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas builds on numpy \n",
    "# See: \"Why is numpy fast?\" https://numpy.org/doc/stable/user/whatisnumpy.html\n",
    "import numpy as np\n",
    "a = np.random.rand(100000)\n",
    "b = np.random.rand(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_slow(a,b):\n",
    "  dot = 0\n",
    "  for i in range(len(a)):\n",
    "    dot += a[i] * b[i]\n",
    "  return dot\n",
    "\n",
    "%timeit dot_product_slow(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.dot(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Futher reading on optimization:\n",
    "- [Locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference)\n",
    "- [Vectorization](https://en.wikipedia.org/wiki/Automatic_vectorization)\n",
    "- [Broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melting and Tidy Data\n",
    "\n",
    "In this part of the demo we will duplicate the slides and get some practice with the Melt command to make a data table tidy where it wasn't before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/religon.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [melt command](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.melt.html) does...\n",
    "\n",
    "```\n",
    "DataFrame.melt(id_vars=None, value_vars=None, var_name=None, value_name='value', col_level=None, ignore_index=True)\n",
    "\n",
    "Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.\n",
    "\n",
    "This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are “unpivoted” to the row axis, leaving just two non-identifier columns, ‘variable’ and ‘value’.\n",
    "```\n",
    "\n",
    "So here we want to take the above data frame and make it `grow down` by converting income into a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.melt(df,\n",
    "               [\"religion\"],\n",
    "               var_name=\"income\",\n",
    "               value_name=\"freq\")\n",
    "f_df = f_df.sort_values(by=[\"income\"])\n",
    "f_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now use more simple slicing to see the things we need.\n",
    "\n",
    "f_df[(f_df['religion'] == 'Atheist')].sort_values(by='income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df[(f_df['income'] == '$50-75k')].sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Over The Top Example\n",
    "\n",
    "In this example we really walk through all the nitty gritty of taking the Billboard data and melting it down, fixing some errors in the table, and getting a dataframe that we can do easier manipulation on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df = pd.read_csv(\"./data/billboard.csv\")\n",
    "b_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This thing is a mess!! What if want to say, graph the positions of a song for a given artist? Then how would we do it?\n",
    "\n",
    "Again, we are going to melt to make the table `grow down` by keeping the `id_vars` and then melting the rows based on week position to be values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep identifier variables\n",
    "id_vars = [\"year\",\n",
    "           \"artist.inverted\",\n",
    "           \"track\",\n",
    "           \"time\",\n",
    "           \"genre\",\n",
    "           \"date.entered\",\n",
    "           \"date.peaked\"]\n",
    "\n",
    "# Melt the rest into week and rank columns\n",
    "b_df = pd.melt(frame=b_df,\n",
    "             id_vars=id_vars,\n",
    "             var_name=\"week\",\n",
    "             value_name=\"rank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(b_df.head(20))\n",
    "b_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fix the week thing, it's not good...\n",
    "b_df[\"week\"] = b_df['week'].str.extract('(\\d+)', expand=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df[\"week\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a subtle problem here, rank is an int but some are NAN's and we can't have [int-nan's...](https://pandas.pydata.org/pandas-docs/version/0.24/whatsnew/v0.24.0.html#optional-integer-na-support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why not ints? Gotcha! ?!?!?\n",
    "b_df[\"rank\"] = b_df[\"rank\"].astype(float)\n",
    "\n",
    "display(b_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see if anything is missing...\n",
    "# Remember axis=1 goes row wise..so we are saying show us any row with missing values...\n",
    "b_df[b_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need these so... let's drop them.\n",
    "b_df = b_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the handy [to_datetime](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) which will get us a date!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"date\" columns\n",
    "#  date = (date entered chart) + (# of weeks) - (1 week) [fence post problem]\n",
    "b_df['date'] = pd.to_datetime(b_df['date.entered']) + pd.to_timedelta(b_df['week'], unit='w') - pd.DateOffset(weeks=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(b_df[100:110])\n",
    "b_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore now-redundant, messy columns -- same as dropping \n",
    "b_df = b_df[[\"year\",\n",
    "         \"artist.inverted\",\n",
    "         \"track\",\n",
    "         \"time\",\n",
    "         \"genre\",\n",
    "         \"week\",\n",
    "         \"rank\",\n",
    "         \"date\"]]\n",
    "\n",
    "b_df = b_df.sort_values(ascending=True, by=[\"year\",\"artist.inverted\",\"track\",\"week\",\"rank\"])\n",
    "\n",
    "# Keep tidy dataset for future usage\n",
    "billboard = b_df\n",
    "\n",
    "billboard.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can see the artists we have...\n",
    "billboard['artist.inverted'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easier to visualize!!\n",
    "b_df[(b_df['artist.inverted'] == 'Jay-Z')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_df[(b_df['artist.inverted'] == 'Jay-Z')]['rank'].plot.hist(bins=100, title=\"Jay-Z Hits by Week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatterplot of JayZ at different songs/days?\n",
    "b_df[(b_df['artist.inverted'] == 'Jay-Z')].plot.scatter(y='rank', \n",
    "                                                        x='date',\n",
    "                                                        rot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g = sns.pairplot(x_vars=[\"date\"], \n",
    "             y_vars=[\"rank\"], \n",
    "             data=b_df[(b_df['artist.inverted'] == 'TLC')], \n",
    "             hue=\"track\",\n",
    "             height=5)\n",
    "\n",
    "# # Hacky rotation?? -- https://github.com/mwaskom/seaborn/issues/867\n",
    "for ax in g.axes.flat:\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_rotation(45)\n",
    "\n",
    "# # Invert..\n",
    "for ax in g.axes.flat:\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can get the list of top ranked songs...\n",
    "b_df[(b_df['rank'] == 1.0)].groupby(['artist.inverted']).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
