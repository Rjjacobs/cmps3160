{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 12 - Probability and Missing Data\n",
    "\n",
    "In this notebook we'll first take a look at what the effect of missing data can be on a classification mode, specifically using the Titanic dataset. Then we'll look at some ways of \"fixing\" that missing data, and the effect it has on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, mount your google drive, change to the course folder, pull latest changes, and change to the lab folder.\n",
    "# Startup Magic to: (1) Mount Google Drive\n",
    "# (2) Change to Course Folder\n",
    "# (3) Pull latest Changes\n",
    "# (4) Move to the Demo Directory so that the data files are available\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/My Drive/cmps3160\n",
    "!git pull\n",
    "%cd _demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes and Standard Magic...\n",
    "### Standard Magic and startup initializers.\n",
    "\n",
    "# Load Numpy\n",
    "import numpy as np\n",
    "# Load MatPlotLib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Load Pandas\n",
    "import pandas as pd\n",
    "# Load SQLITE\n",
    "import sqlite3\n",
    "# Load Stats\n",
    "from scipy import stats\n",
    "\n",
    "# This lets us show plots inline and also save PDF plots if we want them\n",
    "%matplotlib inline\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# These two things are for Pandas, it widens the notebook and lets us display data easily.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# SKLEARN stuff\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Show a ludicrus number of rows and columns\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's load up the Titanic dataset and make a decision tree!\n",
    "# We have a full datasheet here: http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf\n",
    "df_titanic = pd.read_csv(\"./data/titanic.csv\")\n",
    "df_titanic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need a few of these -- Name, Cabin (why?) and home.dest so let's drop those\n",
    "# body is the body id number, so that is only there for those that died, so let's drop it too, boat is life boat number\n",
    "df_titanic.drop(columns=[\"name\",\"cabin\", \"ticket\",\"body\",\"boat\",\"home.dest\"], inplace=True)\n",
    "df_titanic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the easy part first: dummy out sex and embarked, dropna, and go to town.\n",
    "df_ml = df_titanic.dropna()\n",
    "df_ml = pd.get_dummies(df_ml, columns=[\"sex\", \"embarked\"])\n",
    "df_ml.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's MACHINE LEARN!\n",
    "def fit_and_report(df, features, target):\n",
    "    train, test = train_test_split(df, \n",
    "                               test_size=0.4, \n",
    "                               stratify=df[target])\n",
    "    X_train = train[features]\n",
    "    y_train = train[target]\n",
    "    X_test = test[features]\n",
    "    y_test = test[target]\n",
    "    mod_dt = DecisionTreeClassifier(max_depth = 3, random_state = 1)\n",
    "    mod_dt.fit(X_train,y_train)\n",
    "    prediction=mod_dt.predict(X_test)\n",
    "\n",
    "    metrics.plot_confusion_matrix(mod_dt, X_test, y_test,\n",
    "                                 display_labels=[\"died\",\"survived\"],\n",
    "                                 cmap=plt.cm.Blues, normalize='all')\n",
    "    plt.figure(figsize = (15,8))\n",
    "    plot_tree(mod_dt, feature_names = features, class_names={1:\"survived\", 0:\"died\"}, filled = True);\n",
    "\n",
    "    print(f\"The accuracy of the Decision Tree is {metrics.accuracy_score(prediction,y_test):.3f}\")\n",
    "    print(f\"The Precision of the Decision Tree is {metrics.precision_score(prediction,y_test,average='weighted'):.3f}\")\n",
    "    print(f\"The Recall of the Decision Tree is {metrics.recall_score(prediction,y_test,average='weighted'):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_report(df_ml, \n",
    "                ['pclass', 'age', 'sibsp', 'parch', 'fare', 'sex_female', 'sex_male', 'embarked_C', 'embarked_Q', 'embarked_S'], \n",
    "                [\"survived\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we're doing pretty well... but let's see exactly what data is missing..\n",
    "\n",
    "print(len(df_titanic))\n",
    "\n",
    "print(len(df_titanic.dropna()))\n",
    "\n",
    "print(len(df_titanic) - len(df_titanic.dropna()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But what are these features? What's missing? Is it important?\n",
    "df_titanic[df_titanic.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So what kinds of folks are we dropping here?\n",
    "df_titanic[df_titanic.isnull().any(axis=1)].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where are they from?\n",
    "\n",
    "print(df_titanic[df_titanic.isnull().any(axis=1)].embarked.value_counts())\n",
    "print(df_titanic[df_titanic.isnull().any(axis=1)].pclass.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So it looks like we're missing a ton of south hampton folks, that are in third class... \n",
    "# Let's impute with the mean for the moment and see how our results change\n",
    "\n",
    "df_ml_impute = df_titanic.fillna(df_titanic.mean())\n",
    "df_ml_impute.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_impute.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml_impute.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, so now how many are we missing?\n",
    "print(len(df_ml_impute) - len(df_ml_impute.dropna()))\n",
    "\n",
    "# So we still have 2 missing\n",
    "display(df_ml_impute[df_ml_impute.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let Machine Learn again!\n",
    "df_ml_impute = pd.get_dummies(df_ml_impute, columns=[\"sex\", \"embarked\"])\n",
    "fit_and_report(df_ml_impute, \n",
    "                ['pclass', 'age', 'sibsp', 'parch', 'fare', 'sex_female', 'sex_male', 'embarked_C', 'embarked_Q', 'embarked_S'], \n",
    "                [\"survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic[\"age\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic[\"age\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.groupby(\"sibsp\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The old Notebook\n",
    "\n",
    "\n",
    "**Note to Nick/Aron**: This notebook is still very TBD -- not sure if we want to focus more on missing data (i.e., not interpreting results correctly) or fuzzywuzzy? Or maybe something else...\n",
    "\n",
    "\n",
    "This first part is a copy of the missing data notebooks first...\n",
    "\n",
    "\n",
    "In this notebook we will look at the effects missing data can have on conclusions you can draw from data.  We will also go over some practical implementations for linear regressions in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this work we will be using data from: Generalized body composition prediction equation for men using simple measurement techniques\", K.W. Penrose, A.G. Nelson, A.G. Fisher, FACSM, Human Performance research Center, Brigham Young University, Provo, Utah 84602 as listed in Medicine and Science in Sports and Exercise, vol. 17, no. 2, April 1985, p. 189.\n",
    "\n",
    "[Data availabe here.](http://staff.pubhealth.ku.dk/~tag/Teaching/share/data/Bodyfat.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Penrose Data\n",
    "df_penrose = pd.read_csv(\"./data/bodyfat.csv\")\n",
    "\n",
    "display(df_penrose.head())\n",
    "# observations = ['Neck', 'Chest', 'Abdomen', 'Hip', 'Thigh', 'Knee', 'Ankle', 'Biceps', 'Forearm', 'Wrist']\n",
    "observations = ['Age', 'Neck', 'Forearm', 'Wrist']\n",
    "\n",
    "len(df_penrose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do some really basic scatter plotting...\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "\n",
    "for i,o in enumerate(observations):\n",
    "    df_penrose.plot.scatter(x=o, y='bodyfat', ax=ax[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to look at some linear regressions of single variables to see what is going on!  So let's plot some regression lines.  Note that there are at least a few different ways -- [linregress](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html), [polyfit](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html), and [statsmodels](https://www.statsmodels.org/stable/index.html).\n",
    "\n",
    "Here's a good article about it [Data science with Python: 8 ways to do linear regression and measure their speed](https://www.freecodecamp.org/news/data-science-with-python-8-ways-to-do-linear-regression-and-measure-their-speed-b5577d75f8b/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a basic Linear Regression on a Single Variable.\n",
    "# Note that linregress p-value is whether or not the slope is 0, not if the correlation is significant.\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "\n",
    "for i,o in enumerate(observations):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(df_penrose[o],\n",
    "                                                                   df_penrose['bodyfat'])\n",
    "\n",
    "    # Pack these into a nice title\n",
    "    diag_str = \"p-value =\" + str(round(p_value, 7)) + \"\\n\" + \"r-value =\" + str(round(r_value, 7)) + \"\\nstd err. =\" + str(round(std_err, 7))\n",
    "    df_penrose.plot.scatter(x=o, y='bodyfat', title=diag_str, ax=ax[i])\n",
    "    \n",
    "    # Make points and line\n",
    "    pts = np.linspace(df_penrose[o].min(), df_penrose[o].max(), 500)\n",
    "    line = slope * pts + intercept\n",
    "    ax[i].plot(pts, line, lw=1, color='red')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also use the polyfit function\n",
    "\n",
    "# Let's try to fit a linear model with PolyFit.\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "\n",
    "for i,o in enumerate(observations):\n",
    "    # Fit our curve\n",
    "    x1, intercept = np.polyfit(df_penrose[o],df_penrose['bodyfat'], 1)\n",
    "    \n",
    "    # Plot regular points\n",
    "    df_penrose.plot.scatter(x=o, y='bodyfat', ax=ax[i])\n",
    "    \n",
    "    # Plot curve\n",
    "    pts = np.linspace(df_penrose[o].min(), df_penrose[o].max(), 500)\n",
    "    line = x1 * pts + intercept\n",
    "    ax[i].plot(pts, line, lw=1, ls='-', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try fitting a degree 2 polynomial with polyfit.\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "\n",
    "for i,o in enumerate(observations):\n",
    "    \n",
    "    # Fit the polynomial.\n",
    "    x2, x1, intercept = np.polyfit(df_penrose[o],df_penrose['bodyfat'], 2)\n",
    "\n",
    "    # Plot our points.\n",
    "    df_penrose.plot.scatter(x=o, y='bodyfat', ax=ax[i])\n",
    "    \n",
    "    # Plot the Regression Line..\n",
    "    pts = np.linspace(df_penrose[o].min(), df_penrose[o].max(), 500)\n",
    "    line = x2 * pts**2 + x1 * pts + intercept\n",
    "    ax[i].plot(pts, line, lw=1, ls='-', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try fitting a degree 5 polynomial with polyfit.\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "\n",
    "for i,o in enumerate(observations):\n",
    "    \n",
    "    # Fit the polynomial.\n",
    "    x5, x4, x3, x2, x1, intercept = np.polyfit(df_penrose[o],df_penrose['bodyfat'], 5)\n",
    "\n",
    "    # Plot our points.\n",
    "    df_penrose.plot.scatter(x=o, y='bodyfat', ax=ax[i])\n",
    "    \n",
    "    # Plot the Regression Line..\n",
    "    pts = np.linspace(df_penrose[o].min(), df_penrose[o].max(), 500)\n",
    "    line = x5 * pts**5 + x4 * pts**4 + x3 * pts**3 + x2 * pts**2 + x1 * pts + intercept\n",
    "    ax[i].plot(pts, line, lw=1, ls='-', color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Complicated example with Statsmodels.\n",
    "\n",
    "Statsmodels (you'll likely need to install it) gives a much more R-like interface to linear modeling.  You can read [more about it here](https://www.statsmodels.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "df_ind = df_penrose[['Neck', 'Wrist']]\n",
    "df_target = df_penrose['bodyfat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_ind\n",
    "y = df_target\n",
    "\n",
    "# Note the difference in argument order\n",
    "# Call: endog, then exog (dependent, indepenednt)\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) # make the predictions by the model\n",
    "# Print out the statistics\n",
    "model.summary()\n",
    "#fig, ax = plt.subplots(figsize=(12,8))\n",
    "#fig = sm.graphics.plot_partregress(endog=\"bodyfat\", exog_i=['Abdomen', 'Neck'], exog_others='', data=df_penrose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the [single regressor plot](https://tedboy.github.io/statsmodels_doc/generated/statsmodels.graphics.api.plot_partregress.html#statsmodels.graphics.api.plot_partregress)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.regressionplots import plot_partregress\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plot_partregress(endog='bodyfat', exog_i='Neck', exog_others='', data=df_penrose, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have multiple elements in our regression then we need to use a different plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple regression plot\n",
    "from statsmodels.graphics.regressionplots import plot_partregress_grid\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plot_partregress_grid(model, fig=fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to work with regressions and their plots is using the [Seaborn Regression Package](https://seaborn.pydata.org/tutorial/regression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do simple exploratory plots\n",
    "import seaborn as sns\n",
    "df_test = df_penrose.sample(frac=0.10, replace=False)\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "\n",
    "for i,o in enumerate(observations):\n",
    "    sns.regplot(x=o, y='bodyfat', data=df_test, ax=ax[i])\n",
    "    #g.axes.set_xlim(df_test[o].min()*.95,df_test[o].max()*1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice simulator to play with is [this one](https://ndirienzo.shinyapps.io/linear_regression_sim/) which is from [Prof. Nicholas DiRienzo](https://ischool.arizona.edu/people/nicholas-dirienzo) from ASU's School of Information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "We can use sklearn to do a quick logistic regression.  Remember that for logistic regression we are testing whether or not something is true, so we need to add a variable to our data.\n",
    "\n",
    "Someone is obese if their body fat is >32% so we'll add a dummy for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penrose['obese'] = df_penrose.apply(lambda x: 1 if x['bodyfat'] > 32 else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penrose.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to use sklearn to build us a classifier.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# setup our data for testing and training.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_penrose[observations],\n",
    "                                                    df_penrose['obese'],\n",
    "                                                    test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit that model!\n",
    "logisticRegr = LogisticRegression(max_iter=100000, class_weight='balanced') \n",
    "model = logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and plot!\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy Score is: {accuracy_score(y_test, y_pred)}\")\n",
    "plot_confusion_matrix(model, X_test, y_test,\n",
    "                                 display_labels=logisticRegr.classes_,\n",
    "                                 cmap=plt.cm.Blues, normalize='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now back to Missing Data!!\n",
    "\n",
    "What happens if we start to remove parts of the data -- is the relationship still as strong?\n",
    "\n",
    "We can use the [pandas sample command](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html) to remove some of the dataframe.\n",
    "\n",
    "Note that here we are just asking the question, if we took some of the data out randomly, do we still get the same result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a basic Linear Regression on a Single Variable.\n",
    "# Note that linregress p-value for the null-hyp that slope = 0.\n",
    "df_test = df_penrose.sample(frac=0.2, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,5))\n",
    "for i,o in enumerate(observations):\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(df_test[o],\n",
    "                                                                   df_test['bodyfat'])\n",
    "\n",
    "    # Pack these into a nice title\n",
    "    diag_str = \"p-value =\" + str(round(p_value, 7)) + \"\\n\" + \"r-value =\" + str(round(r_value, 7)) + \"\\nstd err. =\" + str(round(std_err, 7))\n",
    "    df_test.plot.scatter(x=o, y='bodyfat', title=diag_str, ax=ax[i])\n",
    "    \n",
    "    # Make points and line\n",
    "    pts = np.linspace(df_test[o].min(), df_test[o].max(), 500)\n",
    "    line = slope * pts + intercept\n",
    "    ax[i].plot(pts, line, lw=1, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to determine if these correlations are significant under the missing data then we need to run bootstrap samples and see what happens.\n",
    "\n",
    "Nick -- modify this to drop part of the data then resample from the dropped part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {o:[] for o in observations}\n",
    "bootstrap_samples = 1000\n",
    "fraction = 0.20\n",
    "\n",
    "for i,o in enumerate(observations):\n",
    "    for t in range(bootstrap_samples):\n",
    "        df_test = df_penrose.sample(frac=fraction, replace=False)\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(df_test[o],df_test['bodyfat'])\n",
    "        #r,p = stats.pearsonr(df_test[o], df_test['bodyfat'])\n",
    "        results[o].append(p_value)\n",
    "        \n",
    "rs = pd.DataFrame(results)\n",
    "ax = rs.boxplot()\n",
    "ax.set_ylim([-0.01,0.30])\n",
    "ax.set_title(f\"p-value of 1 variable regression over {bootstrap_samples} iterations\")\n",
    "ax.set_ylabel(\"p-value\")\n",
    "ax.set_xlabel(\"Body Part\")\n",
    "ax.axhline(y=0.05, lw=2, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above as we run more and more samples and plot the p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Probability and Simulation\n",
    "\n",
    "In this notebook we look at some fun ways to do sampling and test some of the basics of probability just for giggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes and Standard Magic...\n",
    "### Standard Magic and startup initializers.\n",
    "\n",
    "# Load Numpy\n",
    "import numpy as np\n",
    "# Load MatPlotLib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# Load Pandas\n",
    "import pandas as pd\n",
    "# Load Stats\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "# This lets us show plots inline and also save PDF plots if we want them\n",
    "%matplotlib inline\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "matplotlib.style.use('fivethirtyeight')\n",
    "\n",
    "# These two things are for Pandas, it widens the notebook and lets us display data easily.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Show a ludicrus number of rows and columns\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Supress scientific notation\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability and Code!\n",
    "\n",
    "Note we're using [Numpy's probability functions](https://numpy.org/doc/stable/reference/random/index.html), you could also use [Python's](https://docs.python.org/3/library/random.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a probability distribution:\n",
    "outcomes = list(range(1,7))\n",
    "outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate an outcome..\n",
    "np.random.choice(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it a lot...\n",
    "np.random.choice(outcomes, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph it!\n",
    "results = pd.DataFrame(np.random.choice(outcomes, 1000))\n",
    "results.plot.hist(bins=np.arange(0.5,7.5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it with a biased coin..\n",
    "b = 1.0 / 7.0\n",
    "b1 = 2.0 / 7.0\n",
    "results = pd.DataFrame(np.random.choice(outcomes, 1000, p=[b, b, b1, b, b, b]))\n",
    "results.plot.hist(bins=np.arange(0.5,7.5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it for multiple events!\n",
    "die1 = np.random.choice(outcomes, 10000)\n",
    "die2 = np.random.choice(outcomes, 10000)\n",
    "results = pd.DataFrame({'Die1': die1, 'Die2':die2})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add them up...\n",
    "plt.figure(figsize = (12,5))\n",
    "results['sum'] = results[\"Die1\"] + results[\"Die2\"]\n",
    "results['sum'].plot.hist(bins=np.arange(1.5, 13.5, 1), density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default is with replacement but we can do without replacement..\n",
    "people = ['Winona', 'Xanthippe', 'Yvonne', 'Zelda']\n",
    "np.random.choice(people, 3, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One thing we may want to do in a stats model is ....\n",
    "\n",
    "See if a particular distribution is the same as some known distribution. To do this we typically use the [Chi Squared test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html) if we known the underlying distribution.\n",
    "\n",
    "Here we know that rolling two dice and summing them **should** give us a normal distribution so we can use a more complex [normal test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html) from Pearson (of correlation coefficient fame) to check...\n",
    "\n",
    "**Returns**\n",
    "\n",
    "statistic : float or array\n",
    "s^2 + k^2, where s is the z-score returned by skewtest and k is the z-score returned by kurtosistest.\n",
    "\n",
    "pvalue: float or array\n",
    "A 2-sided chi squared probability for the hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if one die is really a uniform distribution.\n",
    "#\n",
    "# Note that we need: f_obsarray_like\n",
    "#                    Observed frequencies in each category.\n",
    "#                    f_exparray_like, optional\n",
    "#Expected frequencies in each category. By default the categories are assumed to be equally likely.\n",
    "#\n",
    "\n",
    "from scipy import stats\n",
    "chisq, p = stats.chisquare(results[\"Die1\"].value_counts(), [1./6, 1./6, 1./6, 1./6, 1./6, 1./6])\n",
    "print(\"ChiSq = {} and p = {}\".format(chisq,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if the sum is normal..\n",
    "\n",
    "from scipy import stats\n",
    "k2, p = stats.normaltest(results[\"sum\"])\n",
    "print(\"K2 = {} and p = {}\".format(k2,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could things like this be used for when building statistical models? [Hint!](http://data8.org/materials-sp18/lec/lec16PDF.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Two Variables.\n",
    "\n",
    "Let's roll two dice a bunch of times and see the resutls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "die1 = np.random.choice(outcomes, 100)\n",
    "die2 = np.random.choice(outcomes, 100)\n",
    "results = pd.DataFrame({'Die1': die1, 'Die2':die2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.crosstab(results['Die1'], results['Die2'])\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = pd.crosstab(results['Die1'], results['Die2'], normalize=True)\n",
    "joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can roll this up for either die to see it's distribution\n",
    "joint.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also get marginals directly.\n",
    "marginals = pd.crosstab(results['Die1'], results['Die2'], normalize=True, margins=True)\n",
    "marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, if we want conditional distributions we have to do a bit of work. Let's try to work out\n",
    "# P(Die 1 is a 6 | Die 2 is a 5)\n",
    "\n",
    "counts = pd.crosstab(results['Die1'], results['Die2'])\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to get the (Die 2 is a 5 row) and then look at the distribution there..\n",
    "\n",
    "counts[5] / counts[5].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Simulation to Answer Probability Questions.\n",
    "\n",
    "In CMPS 2170 we figured out closed form formulas for a set of mutually independent Bernoilli Trials.\n",
    "\n",
    "* Bernoulli Trial: an experiment with two possible outcomes\n",
    "* E.g., flip a coin results in two possible outcomes: head (𝐻) and tail (𝑇)\n",
    "* Independent Bernoulli Trials: a sequence of Bernoulli trails that are mutually independent\n",
    "\n",
    "* Example: What is the probability of the sequence HHHTT for a coin flip sequence with $p$ for H and $1-p$ for T?\n",
    "  * $p^3(1-p)^2$.\n",
    "\n",
    "Recall: The probability of exactly $k$ successes in $n$ independent Bernoulli trials, with probability of success $p$ and probability of failure $q = 1 − p$, is $C(n,k)p^kq^{n-k}$ where $C(n,k)$ is $n$ choose $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a biased coin and flip it a bunch..\n",
    "coin_results = np.random.choice([\"Heads\", \"Tails\"], 100, p=[0.75, 0.25])\n",
    "coin_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A More complex Question..\n",
    "\n",
    "* What is the probability of getting 60 or more heads if I flip 100 coins?\n",
    "* Approximation through simulation:\n",
    "  1. Figure out how to do one experiment (i.e., flip 100 coins).\n",
    "  2. Run the experiment a bunch of times.\n",
    "  3. Find the fraction of times where number of heads >= 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip 100 coins and count heads...\n",
    "coin_results = np.random.choice([\"Heads\", \"Tails\"], 100, p=[0.75, 0.25])\n",
    "print(coin_results == 'Heads')\n",
    "print(np.count_nonzero(coin_results == 'Heads'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap it up and do it a bunch...\n",
    "# Note we're using Numpy here for broadcasting -- numpy arrays are imuteable so \n",
    "# it's a tad more akaward in places..\n",
    "n_reps = 10000\n",
    "\n",
    "def exp():\n",
    "    coin_results = np.random.choice([\"Heads\", \"Tails\"], 100, p=[0.50, 0.50])\n",
    "    return np.count_nonzero(coin_results == 'Heads')\n",
    "\n",
    "head_counts = np.array([])\n",
    "for i in range(n_reps):\n",
    "    head_counts = np.append(head_counts, exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure it out...\n",
    "print(np.count_nonzero(head_counts >= 60))\n",
    "print(np.count_nonzero(head_counts >= 60) / n_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we work out the math we need at least 60 H so we have to add up quite a few things...\n",
    "$\\sum^{100}_{k=60} C(100, k)p^kq^n-k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using simiulation we can also look at the trials\n",
    "n_reps = 1000\n",
    "\n",
    "head_counts = np.array([])\n",
    "for i in range(n_reps):\n",
    "    head_counts = np.append(head_counts, exp())\n",
    "\n",
    "results = pd.DataFrame(head_counts)\n",
    "ax = results.plot.hist()\n",
    "plt.xlim(20,70)\n",
    "plt.axvline(np.mean(head_counts), color='red')\n",
    "plt.title(f\"Mean: {np.mean(head_counts)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settle the Monty Hall Thing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_monty_hall():\n",
    "    behind_picked_door = np.random.choice(['Car', 'Goat 1', 'Goat 2'])\n",
    "    \n",
    "    if behind_picked_door == 'Car':\n",
    "        winning_strategy = 'Stay'\n",
    "    else:\n",
    "        winning_strategy = 'Switch'\n",
    "        \n",
    "    print(behind_picked_door, 'was behind the door. Winning strategy:', winning_strategy)\n",
    "    return winning_strategy\n",
    "simulate_monty_hall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it a bunch...\n",
    "n_repetitions = 10000\n",
    "\n",
    "winning_strategies = np.array([])\n",
    "for i in np.arange(n_repetitions):\n",
    "    winning_strategy = simulate_monty_hall()\n",
    "    winning_strategies = np.append(winning_strategies, winning_strategy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(winning_strategies == 'Switch') / n_repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(winning_strategies == 'Stay') / n_repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
